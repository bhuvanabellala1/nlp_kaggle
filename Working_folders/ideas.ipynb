{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's only try on noun phrase chunks\n",
    "grammar = r\"\"\"\n",
    "  NP: {<JJ.*|NN.*>+ <NN.*|CD>}  # chunk  adjectives and noun\n",
    "    {<N.*>+}                # chunk sequences of nouns\n",
    "\"\"\"\n",
    "count = 1\n",
    "def chunks(text):\n",
    "    \n",
    "    global count\n",
    "    if(count % 100 == 0):\n",
    "        print(count)\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    sentences = [nltk.word_tokenize(sent) for sent in sentences]\n",
    "    sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "    chunk = []\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    for sentence in sentences:\n",
    "        result = cp.parse(sentence)\n",
    "        chunk.append(result)\n",
    "    \n",
    "    count += 1\n",
    "    just_chunks = []\n",
    "    for tree in chunk:\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'NP': \n",
    "                #print(subtree)\n",
    "                phrase = \"\"\n",
    "                for leaf in subtree.leaves():\n",
    "                    phrase += leaf[0].lower() + \" \"\n",
    "                just_chunks.append(phrase.strip())\n",
    "    return just_chunks\n",
    "            \n",
    "#building a vector tokenizer\n",
    "pos_vec = CountVectorizer(ngram_range=(1, 1), tokenizer=chunks, min_df=3, stop_words='english', max_features=1000)  \n",
    "#tokenizer = vec.build_tokenizer()\n",
    "\n",
    "#pos_tfidf_vec = TfidfVectorizer(ngram_range=(1, 1), tokenizer=pos_tokenize, min_df=3, stop_words='english', max_features=1000)\n",
    "#tfidf_tokenizer = tfidf_vec.build_tokenizer()\n",
    "\n",
    "pos_arr_train_feature_sparse = pos_vec.fit_transform(df_train['Review Text'])\n",
    "pos_arr_train_feature = arr_train_feature_sparse.toarray()\n",
    "print(\"the size of the vector the training set is as follows: {}\".format(arr_train_feature.shape))\n",
    "\n",
    "pos_arr_dev_feature_sparse = pos_vec.transform(df_dev[\"Review Text\"])\n",
    "pos_arr_dev_feature = arr_dev_feature_sparse.toarray()\n",
    "print(\"the size of the vector the development set is as follows: {}\".format(arr_dev_feature.shape))\n",
    "\n",
    "pos_arr_test_feature_sparse = pos_vec.transform(df_test['Review Text'])\n",
    "pos_arr_test_feature = arr_test_feature_sparse.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "def tokenize(text):\n",
    "    global count\n",
    "    if(count % 10000 == 0):\n",
    "        print(count)\n",
    "                  \n",
    "    pattern = r'''(?x)     \n",
    "    ([A-Z]\\.)+                       # abbrevations\n",
    "    |\\w+'\\w+                         # contractions\n",
    "    | \\w+(?:(-|(\\.\\s))(\\n)?\\w+)*     # words w/ internal hyphens, extend to next line, and with periods like Mrs. Reed\n",
    "    '''\n",
    "                  \n",
    "    tokens = nltk.regexp_tokenize(text, pattern)\n",
    "    #tokens = nltk.word_tokenize(text)\n",
    "    word_tokens = [word for word in tokens ]\n",
    "    count = count + 1\n",
    "    return word_tokens\n",
    "\n",
    "#building a vector tokenizer\n",
    "vec = CountVectorizer(ngram_range=(1, 3), tokenizer=tokenize, min_df=3, stop_words='english', max_features=1000)  \n",
    "tokenizer = vec.build_tokenizer()\n",
    "\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1, 3), tokenizer=tokenize, min_df=3, stop_words='english', max_features=1000)\n",
    "#tfidf_tokenizer = tfidf_vec.build_tokenizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
